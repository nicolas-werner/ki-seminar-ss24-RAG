{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ollama/ollama/blob/main/examples/langchain-python-rag-document/main.pymm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import OnlinePDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import GPT4AllEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = OnlinePDFLoader(\"https://d18rn0p25nwr6d.cloudfront.net/CIK-0001813756/975b3e9b-268e-4798-a9e4-2a9a7c92dc10.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "with SuppressStdout():\n",
    "    vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nQuery: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    if query.strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Prompt\n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Use three sentences maximum and keep the answer as concise as possible.\n",
    "    {context}\n",
    "    Question: {question}\n",
    "    Helpful Answer:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "    llm = Ollama(model=\"llama3:8b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "    )\n",
    "\n",
    "    result = qa_chain({\"query\": query})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
